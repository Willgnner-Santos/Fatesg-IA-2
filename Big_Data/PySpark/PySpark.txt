---------- CÉLULA 1 ----------
Na primeira celula podemos ver o comando "!" para ser executado pelo Jupyter e não ao python, esse comando ser para conversar com a máquina para ela executar diretamente o sistema operacional com os comandos digitados. O comando " ! apt-get update " atualiza a lista de pacotes que o sistema conhece, permitindo que o sistema ja saiba quais pacotes existem e suas versões mais novas. Depois disso ele coloca um comando para que o sistema instale o Java, porque o Spark precisa dele para rodar o sistema, serve como uma ponte para que ele traduza os comandos em python para o Spark e devolve os resultados pro python (O Pyspark nada mais que uma versão em python do Apache Spark), o motivo que o Spark precisar do Java é porque
é uma máquina que fala apenas " Língua Java ", podemos entender tambem que o Pyspark é como se fosse um tradutor simultâneo que entende python e conversa com a máquina em Java.
Entretanto iremos mudar para python que vai primeiro importar para que o python interaja com o sistema operacional. Com o " update-alternatives " permite escolher qual versão do Java o sistema operacional vai usar como padrão. Feito isso ele ira pedir para instalar a biblioteca pyspark e depois ele ira clonar o repositório que estão no github (um site para desenvolvedores), que estão dentro de um arquivo, para pegar esses arquivos ele ira clonar o repertório usando o comando " git clone " que pode ser imaginado como a nuvem que está guardado o projeto que dentro dessa pasta vem todos os arquivos, códigos  e histórico do projeto que esta no GitHub que você irá rodar, alterar, abrir e até contribuir de volta para o
projeto. Depois o usuário ira colocar um comando para que  os arquivos serem lidos pelo sistema e transformando em arquivos .gz que serve pra ocupar menos espaço no disco que compacta um único arquivo por vez, que é diferente de um arquivo .zip que é pra juntar vários arquivos.
O usuário para entender o tamanho do arquivo ele usará o comando "du -h" para ver o tamanho dos arquivos que ira mostrar o uso do disco rígido pra que possa entender se ele é pequeno ou grande para você trabalhar com ele tendo uma noção de peso de arquivo. O comando "gunzip" serve para descompactar os arquivos, transformando em um arquivo .txt normal, permitindo que outros programas conseguirem ler os arquivos de texto direto. O arquivo "wc -i" serve para ler quantas linhas tem no texto, cada linha representa um acesso registrado e o arquivo "head -n10" serve para ler as 10 primeiras linhas.

---------- CÉLULA 2 ----------
O RDD (dados distribuídos resiliente ) é a estrutura de dados principal que o Spark usa para processar informações em larga escala, como resolver problemas de escala, velocidade e tolerância a falhas. É um conjunto de dados que pode ser pensado como uma lista grande ou uma tabela cheia de registro e que são distribuídos não so em um único computador, por ser impossível de processar em um único computador com tantas informações em uma máquina so, permitindo que o Spark divide esse conjunto de dados em partições com cada participação de diferentes computadores do cluster que serão executadas em várias maquinas ao mesmo tempo e cada cluster cuida de uma parte, no final o Spark junta os resultados. O Spark ele é resiliência 
que quer dizer que é tolerante a falhas, se a parte de máquinas falhou, o Spark recria essa parte automaticamente em outra máquina sem precisar reiniciar todo o processo e ele permite ser imutável que significa que não pode mudar diretamente os dados dentro dele. O RDD é usado quando o controle de baixo nível, dados não estruturados, não precisa de colocar colunas para organizar e pode ter preferência por programações por escrita e não por expressões SQL.

---------- CÉLULA 3 ----------
O SparkConf é um objeto de configuração do Spark que permite onde rodar e quanto de memória usar e o SparkContext é o ponto de entrada do Spark na API de baixo nivel que conecta o programa python ao motor Spark com o operador para somar valores.
Na etapa de configuração do Spark, utilizamos o objeto SparkConf para definir como a aplicação será executada. Primeiro, ao chamar "SparkConf", criamos um objeto vazio que servirá como base para receber as configurações. Em seguida, com "setMaster("local")", informamos que a execução será feita no modo local, ou seja, dentro do próprio computador, usando apenas uma thread. Existe também a opção "local[*]", que permite aproveitar todos os núcleos de processamento disponíveis na máquina, garantindo maior desempenho nos testes. Depois disso, utilizamos ".setAppName("Exercicio Nasa Logs")" para atribuir um nome à aplicação. Esse nome é exibido tanto na interface do Spark quanto nos registros de execução (logs), facilitando 
a identificação do programa em ambientes com múltiplas aplicações.
Por fim, o comando ".set("spark.executor.memory", "5g")" determina que cada executor terá 5 GB de memória para rodar as tarefas. No modo local, isso significa apenas um executor com essa quantidade de memória. Porém, em um cluster real, cada executor que participar da execução também receberia os mesmos 5 GB, o que garante um uso controlado e equilibrado de recursos em larga escala. Depois disso criamos o "SparkContext" para configurar o tamanho de definir como objeto "sc" para que você usa para criar RDDs e rodar operações no Spark.

---------- CÉLULA 4 ----------
O comando "sc.textFile" permite criar um RDD de string, onde cada elemento de RDD é uma linha do arquivo e a leitura verdadeira sera quando você chamar a ação. O cache sera para guardar o RDD na memoria RAM depois da primeira vez que for executado, dizendo para o Spark rodar várias ações e reler e recalcular o arquivo inteiro e ele so le so uma vez e já vai deixar guardado na memória. A Inspeção manual é o comando que mostra as 3 primeiras linhas do arquivo original, isso vai ajudar a inspecionar rapidamente o formato dos dados.

---------- CÉLULA 5 ----------
A variavel foi criada chamada "exemplo", que contem uma linha que representada como uma strin, isso facilitara o processo de analise que podemos treinar a lógica em uma linha ates de aplicar ao arquivo inteiro. Depois é usado o método ".split()" que divide a string em uma lista de pedaços que sera encontrada em branco, sera uma lista de tokens onde cada posição da lista representa uma parte da linha. Utilizando a indexação com cochetes que sera possivel acessar qualquer um desses tokens com operação que retornar o IP. Com a variavel "Julho" ele ira ler as 5 primeiras linhas para o primeiro token e depois coletar para a inspeção. O Spark percorre as linhas do log que ira dividir cada uma em um tokens e pegar apenas o primeiro
item, uma variação que sera iquivalente as sequencia de operações.

---------- CÉLULA 6 ----------
Ira retornar a quantidadede hosts em diferentes acessos ao servidor que iram pegar o primeiro token da linha e depois ele ira remover usando o comando "distinct()" que forem de pouco uso, para ver quantos sobraram ele usa o comando "count()" para contar quantos hosts sobraram. usando a variavel Julho ele ira distinguir quantos hosts sera calculado diretamente do mes de julho e tambem de agosto.

---------- CÉLULA 7 ----------
Nesta célula, é implementada a função codigo404, cuja finalidade é detectar se uma linha de log corresponde a uma requisição que resultou em erro HTTP 404 – Not Found. Cada linha do log segue uma estrutura típica contendo várias informações: o host de origem, a data e hora da requisição, o tipo de operação (GET, POST, etc.), o recurso solicitado, o protocolo, o código de status HTTP e, por fim, o número de bytes transferidos. Nesse padrão, o penúltimo token da linha representa o código de status, enquanto o último corresponde ao tamanho da resposta. A função utiliza o método .split() para dividir a linha em tokens e, com o índice negativo [-2], acessa diretamente o penúltimo elemento. Caso esse valor seja igual a "404",
a função retorna True, indicando que a linha representa um erro de recurso não encontrado.
Para garantir robustez, a função envolve esse processamento em um bloco try/except, o que evita falhas no caso de linhas malformadas, incompletas ou que não possuam o número esperado de tokens. Assim, mesmo que o log contenha inconsistências, a análise não é interrompida. Após a definição, a função é testada em exemplos práticos: primeiro em uma linha com status 200, onde o retorno esperado é False, mostrando que não houve erro; depois em uma linha criada manualmente com status 404, onde o retorno é True, confirmando que a lógica da função está correta.Em resumo, esta célula mostra como construir uma função segura e reutilizável para identificar erros 404 nos registros de acesso, preparando a base para filtrar grandes
volumes de dados de forma confiável dentro do Spark.

---------- CÉLULA 8 ----------
Nesta célula, o objetivo é identificar e contar os erros 404 nos logs de julho e agosto. Para julho, é usada a função codigo404, que é mais robusta pois trata possíveis linhas malformadas, e o resultado é armazenado em cache para evitar recomputações. Já em agosto, utiliza-se uma lambda mais simples, que verifica se o penúltimo token da linha é igual a “404”, mas essa abordagem é menos segura porque pode falhar se a linha não tiver o formato esperado. Por fim, com .count(), o Spark executa a ação e retorna a quantidade total de erros 404 em cada mês.

---------- CÉLULA 9 ----------
O foco é extrair os dados e contar as URLs associadas aos erros 404 nos logs. Primeiro, é usado o exemplo de linha para mostrar como o método .split('"') divide a string em três partes: antes da requisição, a requisição em si e o trecho depois dela. Essa lógica é aplicada no RDD erros404_julho, permitindo extrair uma amostra de 5 URLs com erro 404. Na sequência, cada URL é mapeada para um par no formato (url, 1), representando uma ocorrência. Esse mapeamento é essencial para poder contabilizar as repetições. Com isso, é construído um pipeline completo: as URLs extraídas são transformadas em pares (url, 1) e depois processadas pelo reduceByKey(add), que soma os valores para cada chave (URL), resultando no total de
ocorrências de cada recurso que gerou erro 404. Por fim, o comando ".take(10)" retorna uma amostra de 10 pares para inspeção, e type(counts) confirma que o resultado é um RDD de tuplas, onde cada tupla contém a URL e o número de vezes que ela apareceu com erro. Em resumo, esta célula mostra passo a passo como partir de uma linha de log, extrair a URL da requisição e, com as transformações do Spark, construir uma contagem de quais páginas mais causaram erros 404, o que é essencial em análises de logs e monitoramento de servidores.

---------- CÉLULA 10 ----------
Calculamos  as 5 URLs com mais erros 404 em um RDD que permite extrair a própria URL para edentificar cada linha, podemos fazer a leitura dos dados para identificar cada palavra com a ordem decrescente.

---------- CÉLULA 11 ----------
nesta célula permite extrair os dados das linhas do log e usá-las para contar quantos erros 404 ocorrem em cada linha. O código mostra coma a linha pode ser dividida a partir de um colchete '[' para isolar a parte da hora e data, e depois usar outro para ficar apenas a data do formato. A criação da função "contador_dias_404" permite que aplique a extração em todo RDD de erros 404 que será mapeada para cada dia para trazer resultados como uma lista de tuplas no python.

---------- CÉLULA 12 ----------
Na célula, ele colocara a ordem alfabética com o python para retornar uma nova lista ordenada, ordenando as dicas por quantidade de erros 404 em forma decrescente com o python que ira retornar uma lista tupla. Chamando a função 'lambda' que permite estar anônima que é sem nome, que será mais usada quando necessicitar de uma função rápida e curta que maioria das vezes  em umka única linha criando uma forma simples sem precisar dar o nome para ela. O comando 'x' no código será uma lista tupla em que sera contagem usando o negativo da contagem para transformar a ordenação padrão que sera de uma forma mais légivel para o usuário usando a função lambda e utilizando a função de python de verdadeiro ou falso (boleano).
Essa ordenação de texto permite reaproveitar a variável 'Agosto' que sera uma lista não ordenada que sera convertida em string com as 10 primeiras linhas.

---------- CÉLULA 13 ----------
Nesta célula, o objetivo é calcular a quantidade total de bytes transferidos nos acessos registrados nos logs da NASA, tanto no mês de julho quanto no de agosto. Para isso, foi criada a função quantidade_bytes_acumulados, que encapsula todo o processo. Dentro dela, é definida uma função auxiliar chamada contador, responsável por analisar cada linha individualmente. O funcionamento do contador é o seguinte: ele utiliza o método .split(" ") para dividir a linha em partes, e acessa o último token da lista ([-1]), que corresponde ao campo BYTES. Esse campo normalmente representa a quantidade de dados transferidos em cada requisição. O valor é então convertido para inteiro com int(...), mas como algumas linhas podem
ter o símbolo '-' (indicando ausência de valor), ou até números inválidos/negativos, o código utiliza um bloco try/except.
Com essa lógica, o Spark aplica o map(contador), ou seja, transforma cada linha do RDD em um valor inteiro representando os bytes daquela requisição. O resultado é um novo RDD formado apenas por números. Em seguida, utiliza-se reduce(add) para somar todos esses inteiros de forma distribuída no cluster. Diferente de reduceByKey, que trabalha com pares chave-valor, aqui o reduce simples é suficiente porque estamos lidando apenas com números.No final, a função retorna o total de bytes acumulados. A célula demonstra o uso imprimindo a quantidade total de bytes para julho e para agosto, fornecendo uma visão geral do volume de dados trafegados em cada mês. Isso é extremamente útil em análises de logs, pois permite
entender a carga de tráfego, identificar picos de consumo e avaliar a eficiência do servidor.

---------- CÉLULA FINAL ----------
Ele ira encerrar o Spark e liberar os locais do cluster.
